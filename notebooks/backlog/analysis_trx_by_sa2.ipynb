{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfce5719-aa5e-4daf-9d4c-8ade9254bed6",
   "metadata": {},
   "source": [
    "### Analyse Transaction by SA2\n",
    "1. Merge SA2 to transaction by postcode\n",
    "2. Check how many unique SA2\n",
    "3. Check for null SA2 values\n",
    "4. Per SA2 aggregate: total_population, median age, the transaction average dollar amount (AOV), transaction frequency, and the number of unique customers, use BPNL % (num_unique_cust/total_population)\n",
    "5. groupby month/weeknum over transaction freq/gmv/profit, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71fc43b-3888-42c8-af7d-6b866dbb5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "from pyspark.sql.functions import countDistinct, col, date_format\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    LongType,\n",
    "    DoubleType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    FloatType\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f70d8181-c311-4dd8-90ba-36602176a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "SparkSession$ does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kn/0h56wsl91tz9kbw4pcdll7zw0000gn/T/ipykernel_1849/1536215589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m spark = (\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAST30034 Project 2 BNPL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.repl.eagerEval.enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.parquet.cacheMetadata\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     getattr(\n\u001b[0;32m--> 275\u001b[0;31m                         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SparkSession$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MODULE$\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                     ).applyModifiableSettings(session._jsparkSession, self._options)\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1720\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1721\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1722\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: SparkSession$ does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "# Start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2 BNPL\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77caf39d-799e-4ef8-b9b5-120db7649202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BNPL dataset\n",
    "consumer = spark.read.csv(\"../data/tables/tbl_consumer.csv\", header=True, sep=\"|\")\n",
    "details = spark.read.parquet(\"../data/tables/consumer_user_details.parquet\")\n",
    "merchants = spark.read.parquet(\"../data/tables/tbl_merchants.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659724b7-751e-4439-9933-f8e08160db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added transactions_20210228_20210827_snapshot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added transactions_20210828_20220227_snapshot\n"
     ]
    }
   ],
   "source": [
    "# load all transactions datasets\n",
    "paths=['../data/tables/transactions_20210228_20210827_snapshot',\n",
    "       '../data/tables/transactions_20210828_20220227_snapshot']\n",
    "\n",
    "first = 1\n",
    "for path in paths:\n",
    "    if first:\n",
    "        transactions = spark.read.parquet(path)\n",
    "        print(f'added {path.split(\"/\")[3]}')\n",
    "        first = 0\n",
    "    else:\n",
    "        append_transactions = spark.read.parquet(path)\n",
    "        transactions = transactions.union(append_transactions)\n",
    "        print(f'added {path.split(\"/\")[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf7f91f-fdbd-49fe-a2ff-4b3adcef5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load poa_to_sa2 dataset\n",
    "poa_to_sa2 = spark.read.csv(\"../data/curated/poa_w_sa2.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d307578c-71b9-4102-9aaf-e8cea0874d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150bf2ae-addd-49a3-a767-cf14264079ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|max(order_datetime)|\n",
      "+-------------------+\n",
      "|         2022-02-27|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|min(order_datetime)|\n",
      "+-------------------+\n",
      "|         2021-02-28|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions.agg({'order_datetime': 'max'}).show()\n",
    "transactions.agg({'order_datetime': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e43775f0-3eea-4787-8c0d-e211c1c71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "merchants = merchants.withColumnRenamed('name', 'merchant_name')\n",
    "consumer = consumer.withColumnRenamed('name', 'consumer_name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b7c8a-b23e-45f0-8745-cf307b85dcbf",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1. Merge SA2 to transaction by postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dbbfd14-868f-4438-8025-9ec0f78beaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join consumers with their respective details\n",
    "consumer_detail = consumer.join(details, on=\"consumer_id\")\n",
    "\n",
    "# Join consumers with their respective transactions\n",
    "consumer_trx = consumer_detail.join(transactions, on=\"user_id\")\n",
    "\n",
    "# Join transactions with the respective merchants\n",
    "df_trx = consumer_trx.join(merchants, on=\"merchant_abn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bd350b-3e62-4849-8b4a-8a4fe42698a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " merchant_abn   | 33064796871          \n",
      " user_id        | 7                    \n",
      " consumer_id    | 511685               \n",
      " consumer_name  | Andrea Jones         \n",
      " address        | 122 Brandon Cliff    \n",
      " state          | QLD                  \n",
      " postcode       | 4606                 \n",
      " gender         | Female               \n",
      " dollar_value   | 373.0873675184212    \n",
      " order_id       | fe188788-b89f-4dd... \n",
      " order_datetime | 2021-08-20           \n",
      " merchant_name  | Curabitur Massa C... \n",
      " tags           | ((computer progra... \n",
      "-RECORD 1------------------------------\n",
      " merchant_abn   | 68435002949          \n",
      " user_id        | 7                    \n",
      " consumer_id    | 511685               \n",
      " consumer_name  | Andrea Jones         \n",
      " address        | 122 Brandon Cliff    \n",
      " state          | QLD                  \n",
      " postcode       | 4606                 \n",
      " gender         | Female               \n",
      " dollar_value   | 232.5364986739752    \n",
      " order_id       | b4a89891-a113-45e... \n",
      " order_datetime | 2021-08-20           \n",
      " merchant_name  | Aliquam Eu Inc.      \n",
      " tags           | [(artist supply a... \n",
      "-RECORD 2------------------------------\n",
      " merchant_abn   | 41944909975          \n",
      " user_id        | 7                    \n",
      " consumer_id    | 511685               \n",
      " consumer_name  | Andrea Jones         \n",
      " address        | 122 Brandon Cliff    \n",
      " state          | QLD                  \n",
      " postcode       | 4606                 \n",
      " gender         | Female               \n",
      " dollar_value   | 30.910755230234322   \n",
      " order_id       | 302ae628-8eba-4a5... \n",
      " order_datetime | 2021-08-20           \n",
      " merchant_name  | Et Nunc Consulting   \n",
      " tags           | ([books, periodic... \n",
      "-RECORD 3------------------------------\n",
      " merchant_abn   | 21439773999          \n",
      " user_id        | 7                    \n",
      " consumer_id    | 511685               \n",
      " consumer_name  | Andrea Jones         \n",
      " address        | 122 Brandon Cliff    \n",
      " state          | QLD                  \n",
      " postcode       | 4606                 \n",
      " gender         | Female               \n",
      " dollar_value   | 91.18655746114226    \n",
      " order_id       | 4524fdc9-73f0-477... \n",
      " order_datetime | 2021-08-21           \n",
      " merchant_name  | Mauris Non Institute \n",
      " tags           | ([cable, satellit... \n",
      "-RECORD 4------------------------------\n",
      " merchant_abn   | 86662713230          \n",
      " user_id        | 7                    \n",
      " consumer_id    | 511685               \n",
      " consumer_name  | Andrea Jones         \n",
      " address        | 122 Brandon Cliff    \n",
      " state          | QLD                  \n",
      " postcode       | 4606                 \n",
      " gender         | Female               \n",
      " dollar_value   | 38.8137172956379     \n",
      " order_id       | 28f9e0f3-858d-445... \n",
      " order_datetime | 2021-08-19           \n",
      " merchant_name  | Vestibulum Accums... \n",
      " tags           | [[watch, clock, a... \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trx.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca8b9d32-f3f4-4fb7-90d2-8f6745266b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- consumer_id: string (nullable = true)\n",
      " |-- consumer_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trx.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0f9db6a-20d9-4467-ab4c-d90c6117a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " poa_code_2016     | 800                  \n",
      " poa_name_2016     | 0800                 \n",
      " sa2_maincode_2016 | 701011002.0          \n",
      " sa2_name_2016     | Darwin City          \n",
      " geometry          | POLYGON ((130.834... \n",
      "-RECORD 1---------------------------------\n",
      " poa_code_2016     | 810                  \n",
      " poa_name_2016     | 0810                 \n",
      " sa2_maincode_2016 | 701021013.0          \n",
      " sa2_name_2016     | Brinkin - Nakara     \n",
      " geometry          | POLYGON ((130.863... \n",
      "-RECORD 2---------------------------------\n",
      " poa_code_2016     | 812                  \n",
      " poa_name_2016     | 0812                 \n",
      " sa2_maincode_2016 | 701021014.0          \n",
      " sa2_name_2016     | Buffalo Creek        \n",
      " geometry          | POLYGON ((130.901... \n",
      "-RECORD 3---------------------------------\n",
      " poa_code_2016     | 815                  \n",
      " poa_name_2016     | 0815                 \n",
      " sa2_maincode_2016 | 701021013.0          \n",
      " sa2_name_2016     | Brinkin - Nakara     \n",
      " geometry          | POLYGON ((130.863... \n",
      "-RECORD 4---------------------------------\n",
      " poa_code_2016     | 820                  \n",
      " poa_name_2016     | 0820                 \n",
      " sa2_maincode_2016 | 701011006.0          \n",
      " sa2_name_2016     | Ludmilla - The Na... \n",
      " geometry          | POLYGON ((130.844... \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poa_to_sa2.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c2604c8-4e1c-45d2-b915-5f566311c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate postcodes in transaction to sa2 codes\n",
    "sa2_cols = ['poa_name_2016', 'sa2_maincode_2016', 'sa2_name_2016', 'geometry']\n",
    "df_trx_sa2 = (df_trx \\\n",
    "                .join(poa_to_sa2[sa2_cols], \n",
    "                     on=[df_trx['postcode'] == poa_to_sa2['poa_name_2016']],\n",
    "                     how='inner')\n",
    "                .drop('poa_name_2016')\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb50a7d-5350-47e2-8c8c-fa3a21200a73",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. Check how many unique SA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae527de5-c447-4bb8-b751-3a08bd60fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1314"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trx_sa2.select('sa2_maincode_2016').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b49e78df-5638-4d1a-a798-17b482af5907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- consumer_id: string (nullable = true)\n",
      " |-- consumer_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- merchant_name: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- sa2_maincode_2016: string (nullable = true)\n",
      " |-- sa2_name_2016: string (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trx_sa2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee4a54-083f-4774-845d-69e91f215d2a",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. Check for null in SA2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1cb6ce0-afac-435e-b80c-4417417ca831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'merchant_abn': 0,\n",
       " 'user_id': 0,\n",
       " 'consumer_id': 0,\n",
       " 'consumer_name': 0,\n",
       " 'address': 0,\n",
       " 'state': 0,\n",
       " 'postcode': 0,\n",
       " 'gender': 0,\n",
       " 'dollar_value': 0,\n",
       " 'order_id': 0,\n",
       " 'order_datetime': 0,\n",
       " 'merchant_name': 0,\n",
       " 'tags': 0,\n",
       " 'sa2_maincode_2016': 0,\n",
       " 'sa2_name_2016': 0,\n",
       " 'geometry': 12090}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_null = {col:df_trx_sa2.filter(df_trx_sa2[col].isNull()).count() \n",
    "             for col in df_trx_sa2.columns}\n",
    "dict_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ce84d-cfc7-440f-9d81-8a7422cf19a6",
   "metadata": {},
   "source": [
    "### 4. Analyse by State (monthly):\n",
    "- by Total Dollar Value, \n",
    "- Active Merchants, \n",
    "- Active Consumers,\n",
    "- AOV (overall, male vs female), \n",
    "- BNPL user % (num_unique_cust/total_population_over_18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35f14325-d114-4daa-a1e1-1496ac429480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " merchant_abn      | 23661821077          \n",
      " user_id           | 13882                \n",
      " consumer_id       | 151968               \n",
      " consumer_name     | Scott Dean           \n",
      " address           | 09010 Brandi Prairie \n",
      " state             | NSW                  \n",
      " postcode          | 2016                 \n",
      " gender            | Male                 \n",
      " dollar_value      | 51.527409870273424   \n",
      " order_id          | f20fdc13-9500-483... \n",
      " order_datetime    | 2021-08-19           \n",
      " merchant_name     | Suspendisse Eleif... \n",
      " tags              | ((computer progra... \n",
      " sa2_maincode_2016 | 117031335.0          \n",
      " sa2_name_2016     | Redfern - Chippen... \n",
      " geometry          | POLYGON ((151.196... \n",
      "-RECORD 1---------------------------------\n",
      " merchant_abn      | 88202878932          \n",
      " user_id           | 13882                \n",
      " consumer_id       | 151968               \n",
      " consumer_name     | Scott Dean           \n",
      " address           | 09010 Brandi Prairie \n",
      " state             | NSW                  \n",
      " postcode          | 2016                 \n",
      " gender            | Male                 \n",
      " dollar_value      | 90.92173922851352    \n",
      " order_id          | 36d57e18-6402-44e... \n",
      " order_datetime    | 2021-08-14           \n",
      " merchant_name     | Tellus Id Limited    \n",
      " tags              | ((gift, card, nov... \n",
      " sa2_maincode_2016 | 117031335.0          \n",
      " sa2_name_2016     | Redfern - Chippen... \n",
      " geometry          | POLYGON ((151.196... \n",
      "-RECORD 2---------------------------------\n",
      " merchant_abn      | 11439466003          \n",
      " user_id           | 13882                \n",
      " consumer_id       | 151968               \n",
      " consumer_name     | Scott Dean           \n",
      " address           | 09010 Brandi Prairie \n",
      " state             | NSW                  \n",
      " postcode          | 2016                 \n",
      " gender            | Male                 \n",
      " dollar_value      | 12.139515327904407   \n",
      " order_id          | b92c9327-8873-460... \n",
      " order_datetime    | 2021-08-14           \n",
      " merchant_name     | Blandit At LLC       \n",
      " tags              | [[shoe shops], [a... \n",
      " sa2_maincode_2016 | 117031335.0          \n",
      " sa2_name_2016     | Redfern - Chippen... \n",
      " geometry          | POLYGON ((151.196... \n",
      "-RECORD 3---------------------------------\n",
      " merchant_abn      | 46987545043          \n",
      " user_id           | 13882                \n",
      " consumer_id       | 151968               \n",
      " consumer_name     | Scott Dean           \n",
      " address           | 09010 Brandi Prairie \n",
      " state             | NSW                  \n",
      " postcode          | 2016                 \n",
      " gender            | Male                 \n",
      " dollar_value      | 69.43691754899224    \n",
      " order_id          | c44a4858-63a2-424... \n",
      " order_datetime    | 2021-08-16           \n",
      " merchant_name     | Lobortis Nisi Nib... \n",
      " tags              | [[cable, satellit... \n",
      " sa2_maincode_2016 | 117031335.0          \n",
      " sa2_name_2016     | Redfern - Chippen... \n",
      " geometry          | POLYGON ((151.196... \n",
      "-RECORD 4---------------------------------\n",
      " merchant_abn      | 96334476428          \n",
      " user_id           | 13882                \n",
      " consumer_id       | 151968               \n",
      " consumer_name     | Scott Dean           \n",
      " address           | 09010 Brandi Prairie \n",
      " state             | NSW                  \n",
      " postcode          | 2016                 \n",
      " gender            | Male                 \n",
      " dollar_value      | 82.67685821098736    \n",
      " order_id          | e0a4ef72-c10c-474... \n",
      " order_datetime    | 2021-08-16           \n",
      " merchant_name     | Sed Est Ltd          \n",
      " tags              | [(bicycle shops -... \n",
      " sa2_maincode_2016 | 117031335.0          \n",
      " sa2_name_2016     | Redfern - Chippen... \n",
      " geometry          | POLYGON ((151.196... \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trx_sa2.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ef3225-90e8-4a33-a172-f05883b2bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx_sa2 = df_trx_sa2.withColumn(\"order_month\", \n",
    "                     date_format(col(\"order_datetime\"), \"M\").cast('INT'))\n",
    "\n",
    "df_trx_sa2 = df_trx_sa2.withColumn(\"order_year\", \n",
    "                     date_format(col(\"order_datetime\"), \"y\").cast('INT'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97524a94-2950-4e40-a72b-9c098e21702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_trx = (df_trx_sa2.groupby(['state', 'order_year', 'order_month'])\n",
    "             .agg({'order_id':'count', 'dollar_value':'sum'})\n",
    "             .sort(['state', 'order_year', 'order_month']))\n",
    "unique_cons = (df_trx_sa2.groupby(['state', 'order_year', 'order_month'])\n",
    "               .agg(countDistinct('consumer_id'))\n",
    "               .sort(['state', 'order_year', 'order_month']))\n",
    "unique_merc = (df_trx_sa2.groupby(['state', 'order_year', 'order_month'])\n",
    "               .agg(countDistinct('merchant_abn'))\n",
    "               .sort(['state', 'order_year', 'order_month']))\n",
    "\n",
    "def join_agg(sdf1, sdf2):\n",
    "    '''\n",
    "        take two dataframes and join the two dataframes\n",
    "    '''\n",
    "    sdf1 = (sdf1.alias(\"a\") \\\n",
    "               .join(sdf2, \n",
    "                     on=['state', 'order_year', 'order_month'], \n",
    "                     how='inner')\n",
    "           )\n",
    "    return sdf1\n",
    "state_trx = join_agg(state_trx, unique_cons)\n",
    "state_trx = join_agg(state_trx, unique_merc)\n",
    "    \n",
    "# renaming a few columns\n",
    "field_name_change = {\"sum(dollar_value)\": \"total_dollar_value\", \n",
    "                     \"count(order_id)\": \"transaction_freq\",\n",
    "                     \"count(consumer_id)\": \"n_unique_consumer\",\n",
    "                     \"count(merchant_abn)\": \"n_unique_merchant\"}\n",
    "for old, new in field_name_change.items():\n",
    "    state_trx = state_trx.withColumnRenamed(old, new)\n",
    "\n",
    "cols = ['state', 'order_year', 'order_month', 'n_unique_consumer', \n",
    "        'transaction_freq', 'total_dollar_value', 'n_unique_merchant']\n",
    "state_trx = state_trx[cols].sort(['state', 'order_year', 'order_month'])\n",
    "\n",
    "state_trx = (state_trx.\n",
    "             withColumn('avg_sales_per_consumer', \n",
    "                        col(\"total_dollar_value\") / col(\"n_unique_consumer\")))\n",
    "\n",
    "state_trx = (state_trx.\n",
    "             withColumn('avg_sales_per_merchant', \n",
    "                        col(\"total_dollar_value\") / col(\"n_unique_merchant\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24b6d268-0ee4-4b63-8106-455e308bc10e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/Users/patrick/Downloads/state_trx.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kn/0h56wsl91tz9kbw4pcdll7zw0000gn/T/ipykernel_1849/2163167929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_trx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/patrick/Downloads/state_trx.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def orc(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/Users/patrick/Downloads/state_trx.csv already exists."
     ]
    }
   ],
   "source": [
    "state_trx.write.csv('/Users/patrick/Downloads/state_trx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6089928-6024-4461-88bd-d7701bd9af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_trx = (df_trx_sa2.groupby(['consumer_id', 'state', 'postcode', \n",
    "                                    'sa2_maincode_2016'])\n",
    "                 .agg({'order_id':'count', 'dollar_value':'sum'})\n",
    "                 .sort(['consumer_id']))\n",
    "\n",
    "consumer_trx = (consumer_trx.\n",
    "             withColumn('aov_consumer', \n",
    "                        col(\"sum(dollar_value)\") / col(\"count(order_id)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4231b00-a65d-4b10-88e9-0f38d170a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>state</th><th>avg(aov_consumer)</th></tr>\n",
       "<tr><td>ACT</td><td>158.7767030922387</td></tr>\n",
       "<tr><td>SA</td><td>158.38187416774943</td></tr>\n",
       "<tr><td>TAS</td><td>159.05346106470913</td></tr>\n",
       "<tr><td>WA</td><td>158.93517295210245</td></tr>\n",
       "<tr><td>QLD</td><td>158.97512088335768</td></tr>\n",
       "<tr><td>VIC</td><td>158.1636956303271</td></tr>\n",
       "<tr><td>NSW</td><td>158.57322324864256</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+------------------+\n",
       "|state| avg(aov_consumer)|\n",
       "+-----+------------------+\n",
       "|  ACT| 158.7767030922387|\n",
       "|   SA|158.38187416774943|\n",
       "|  TAS|159.05346106470913|\n",
       "|   WA|158.93517295210245|\n",
       "|  QLD|158.97512088335768|\n",
       "|  VIC| 158.1636956303271|\n",
       "|  NSW|158.57322324864256|\n",
       "+-----+------------------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average consumer order value per transaction by state\n",
    "consumer_trx.groupby('state').agg({'aov_consumer':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2bf592-4f90-48ed-b457-0e4cdff51018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>consumer_id</th><th>state</th><th>postcode</th><th>sa2_maincode_2016</th><th>sum(dollar_value)</th><th>count(order_id)</th><th>aov_consumer</th></tr>\n",
       "<tr><td>1000031</td><td>NSW</td><td>2177</td><td>127021509.0</td><td>59309.767837269435</td><td>327</td><td>181.3754368112215</td></tr>\n",
       "<tr><td>1000051</td><td>QLD</td><td>4053</td><td>302011025.0</td><td>53789.85710050044</td><td>323</td><td>166.53206532662676</td></tr>\n",
       "<tr><td>1000067</td><td>WA</td><td>6623</td><td>511041291.0</td><td>41416.99769537791</td><td>311</td><td>133.17362603015405</td></tr>\n",
       "<tr><td>1000092</td><td>QLD</td><td>4850</td><td>318011463.0</td><td>56844.87832772903</td><td>351</td><td>161.9512203069203</td></tr>\n",
       "<tr><td>1000115</td><td>WA</td><td>6030</td><td>505031101.0</td><td>53907.42499057463</td><td>354</td><td>152.2808615552956</td></tr>\n",
       "<tr><td>1000286</td><td>SA</td><td>5280</td><td>407021152.0</td><td>49633.87304948021</td><td>330</td><td>150.40567590751579</td></tr>\n",
       "<tr><td>1000293</td><td>VIC</td><td>3242</td><td>217031473.0</td><td>56934.372452776566</td><td>343</td><td>165.98942406057307</td></tr>\n",
       "<tr><td>100039</td><td>NSW</td><td>2118</td><td>125021476.0</td><td>70589.49555532858</td><td>338</td><td>208.84466140629758</td></tr>\n",
       "<tr><td>1000463</td><td>WA</td><td>6082</td><td>504031060.0</td><td>47737.10668715376</td><td>311</td><td>153.4955198943851</td></tr>\n",
       "<tr><td>1000519</td><td>VIC</td><td>3550</td><td>202011020.0</td><td>64012.22817865185</td><td>332</td><td>192.8079162007586</td></tr>\n",
       "<tr><td>1000631</td><td>VIC</td><td>3584</td><td>215031405.0</td><td>49205.1423412053</td><td>328</td><td>150.01567786952833</td></tr>\n",
       "<tr><td>1000706</td><td>SA</td><td>5493</td><td>405031121.0</td><td>53810.01715496924</td><td>330</td><td>163.06065804536132</td></tr>\n",
       "<tr><td>1000718</td><td>SA</td><td>5013</td><td>404021102.0</td><td>43288.49725447742</td><td>331</td><td>130.78095847274145</td></tr>\n",
       "<tr><td>1000858</td><td>WA</td><td>6368</td><td>509021240.0</td><td>42280.788275418316</td><td>342</td><td>123.62803589303601</td></tr>\n",
       "<tr><td>1000878</td><td>SA</td><td>5372</td><td>405011111.0</td><td>46143.186102043794</td><td>323</td><td>142.85816130663713</td></tr>\n",
       "<tr><td>1000958</td><td>SA</td><td>5095</td><td>402041171.0</td><td>47343.52915979767</td><td>331</td><td>143.03181015044612</td></tr>\n",
       "<tr><td>1000999</td><td>SA</td><td>5280</td><td>407021152.0</td><td>54718.204021879275</td><td>322</td><td>169.93231062695426</td></tr>\n",
       "<tr><td>1001013</td><td>QLD</td><td>4875</td><td>315011401.0</td><td>53308.40609186698</td><td>353</td><td>151.01531470783846</td></tr>\n",
       "<tr><td>100102</td><td>NSW</td><td>2750</td><td>124031464.0</td><td>52241.89840810643</td><td>318</td><td>164.2826993965611</td></tr>\n",
       "<tr><td>1001023</td><td>VIC</td><td>3525</td><td>215031400.0</td><td>53544.14502293787</td><td>313</td><td>171.06755598382705</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+-----+--------+-----------------+------------------+---------------+------------------+\n",
       "|consumer_id|state|postcode|sa2_maincode_2016| sum(dollar_value)|count(order_id)|      aov_consumer|\n",
       "+-----------+-----+--------+-----------------+------------------+---------------+------------------+\n",
       "|    1000031|  NSW|    2177|      127021509.0|59309.767837269435|            327| 181.3754368112215|\n",
       "|    1000051|  QLD|    4053|      302011025.0| 53789.85710050044|            323|166.53206532662676|\n",
       "|    1000067|   WA|    6623|      511041291.0| 41416.99769537791|            311|133.17362603015405|\n",
       "|    1000092|  QLD|    4850|      318011463.0| 56844.87832772903|            351| 161.9512203069203|\n",
       "|    1000115|   WA|    6030|      505031101.0| 53907.42499057463|            354| 152.2808615552956|\n",
       "|    1000286|   SA|    5280|      407021152.0| 49633.87304948021|            330|150.40567590751579|\n",
       "|    1000293|  VIC|    3242|      217031473.0|56934.372452776566|            343|165.98942406057307|\n",
       "|     100039|  NSW|    2118|      125021476.0| 70589.49555532858|            338|208.84466140629758|\n",
       "|    1000463|   WA|    6082|      504031060.0| 47737.10668715376|            311| 153.4955198943851|\n",
       "|    1000519|  VIC|    3550|      202011020.0| 64012.22817865185|            332| 192.8079162007586|\n",
       "|    1000631|  VIC|    3584|      215031405.0|  49205.1423412053|            328|150.01567786952833|\n",
       "|    1000706|   SA|    5493|      405031121.0| 53810.01715496924|            330|163.06065804536132|\n",
       "|    1000718|   SA|    5013|      404021102.0| 43288.49725447742|            331|130.78095847274145|\n",
       "|    1000858|   WA|    6368|      509021240.0|42280.788275418316|            342|123.62803589303601|\n",
       "|    1000878|   SA|    5372|      405011111.0|46143.186102043794|            323|142.85816130663713|\n",
       "|    1000958|   SA|    5095|      402041171.0| 47343.52915979767|            331|143.03181015044612|\n",
       "|    1000999|   SA|    5280|      407021152.0|54718.204021879275|            322|169.93231062695426|\n",
       "|    1001013|  QLD|    4875|      315011401.0| 53308.40609186698|            353|151.01531470783846|\n",
       "|     100102|  NSW|    2750|      124031464.0| 52241.89840810643|            318| 164.2826993965611|\n",
       "|    1001023|  VIC|    3525|      215031400.0| 53544.14502293787|            313|171.06755598382705|\n",
       "+-----------+-----+--------+-----------------+------------------+---------------+------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer_trx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a152f92-d68a-4f05-94c2-b14ea9b6c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- poa_code_2016: string (nullable = true)\n",
      " |-- poa_name_2016: string (nullable = true)\n",
      " |-- sa2_maincode_2016: string (nullable = true)\n",
      " |-- sa2_name_2016: string (nullable = true)\n",
      " |-- geometry: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poa_to_sa2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0817ba-581d-4878-9448-805f6061999c",
   "metadata": {},
   "source": [
    "---\n",
    "Analyse income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcd8fa89-dcb8-47ca-aae0-750d7b658ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_income = gpd.read_file(f'../data/abs/sa2_income.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cadd7bb2-eaed-4bff-943b-3e96b00baadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 2288 entries, 0 to 2287\n",
      "Data columns (total 22 columns):\n",
      " #   Column                          Non-Null Count  Dtype   \n",
      "---  ------                          --------------  -----   \n",
      " 0   gml_id                          2288 non-null   object  \n",
      " 1   fid                             2288 non-null   int64   \n",
      " 2   sa2_code                        2288 non-null   int64   \n",
      " 3   sa2_name                        2288 non-null   object  \n",
      " 4   earners_persons                 2241 non-null   float64 \n",
      " 5   median_age_of_earners_years     2241 non-null   float64 \n",
      " 6   sum_aud                         2241 non-null   float64 \n",
      " 7   median_aud                      2241 non-null   float64 \n",
      " 8   mean_aud                        2241 non-null   float64 \n",
      " 9   lowest_quartile_pc              2196 non-null   float64 \n",
      " 10  second_quartile_pc              2196 non-null   float64 \n",
      " 11  third_quartile_pc               2196 non-null   float64 \n",
      " 12  highest_quartile_pc             2196 non-null   float64 \n",
      " 13  percentile_ratos_p80_p20_ratio  2170 non-null   float64 \n",
      " 14  percentile_ratos_p80_p50_ratio  2170 non-null   float64 \n",
      " 15  percentile_ratos_p20_p50_ratio  2170 non-null   float64 \n",
      " 16  percentile_ratos_p10_p50_ratio  2170 non-null   float64 \n",
      " 17  gini_coefficient_coef           2179 non-null   float64 \n",
      " 18  income_share_top_1pc_pc         2147 non-null   float64 \n",
      " 19  income_share_top_5pc_pc         2179 non-null   float64 \n",
      " 20  income_share_top_10pc_pc        2179 non-null   float64 \n",
      " 21  geometry                        2288 non-null   geometry\n",
      "dtypes: float64(17), geometry(1), int64(2), object(2)\n",
      "memory usage: 393.4+ KB\n"
     ]
    }
   ],
   "source": [
    "sa2_income.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60d3da51-cfeb-4886-95a0-9e08ae8a89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_income['sa2_code'] = sa2_income['sa2_code'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a288dca2-9a9a-434b-90a3-4090bd997ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gml_id: string (nullable = true)\n",
      " |-- fid: long (nullable = true)\n",
      " |-- sa2_code: double (nullable = true)\n",
      " |-- sa2_name: string (nullable = true)\n",
      " |-- earners_persons: double (nullable = true)\n",
      " |-- median_age_of_earners_years: double (nullable = true)\n",
      " |-- sum_aud: double (nullable = true)\n",
      " |-- median_aud: double (nullable = true)\n",
      " |-- mean_aud: double (nullable = true)\n",
      " |-- lowest_quartile_pc: double (nullable = true)\n",
      " |-- second_quartile_pc: double (nullable = true)\n",
      " |-- third_quartile_pc: double (nullable = true)\n",
      " |-- highest_quartile_pc: double (nullable = true)\n",
      " |-- percentile_ratos_p80_p20_ratio: double (nullable = true)\n",
      " |-- percentile_ratos_p80_p50_ratio: double (nullable = true)\n",
      " |-- percentile_ratos_p20_p50_ratio: double (nullable = true)\n",
      " |-- percentile_ratos_p10_p50_ratio: double (nullable = true)\n",
      " |-- gini_coefficient_coef: double (nullable = true)\n",
      " |-- income_share_top_1pc_pc: double (nullable = true)\n",
      " |-- income_share_top_5pc_pc: double (nullable = true)\n",
      " |-- income_share_top_10pc_pc: double (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- __geom__: long (nullable = true)\n",
      " |    |-- _is_empty: boolean (nullable = true)\n",
      " |    |-- _ndim: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "sdf_income=spark.createDataFrame(sa2_income) \n",
    "sdf_income.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dff17821-7857-4563-80f7-08dc8df5baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_income = sdf_income.withColumn(\n",
    "                'sa2_code',\n",
    "                F.col('sa2_code').cast('float')\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1698fd0a-d136-497f-875d-b64858a7a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trx_income = df_trx_sa2.join(sa2_income[['sa2_code','median_aud',\n",
    "#                                           'median_age_of_earners_years',\n",
    "#                                           'gini_coefficient_coef',\n",
    "#                                           'earners_persons']], \n",
    "#                               how='left', left_on='sa2_maincode_2016', \n",
    "#                               right_on='sa2_code')\n",
    "trx_income = (df_trx_sa2 \\\n",
    "               .join(sdf_income[['sa2_code','median_aud',\n",
    "                                 'median_age_of_earners_years',\n",
    "                                 'gini_coefficient_coef',\n",
    "                                 'earners_persons']], \n",
    "                     on=[df_trx_sa2['sa2_maincode_2016'] == sdf_income['sa2_code']], \n",
    "                     how='left')\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a70764e2-39b9-4260-ad3d-46282505513b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'merchant_abn': 0,\n",
       " 'user_id': 0,\n",
       " 'consumer_id': 0,\n",
       " 'consumer_name': 0,\n",
       " 'address': 0,\n",
       " 'state': 0,\n",
       " 'postcode': 0,\n",
       " 'gender': 0,\n",
       " 'dollar_value': 0,\n",
       " 'order_id': 0,\n",
       " 'order_datetime': 0,\n",
       " 'merchant_name': 0,\n",
       " 'tags': 0,\n",
       " 'sa2_maincode_2016': 0,\n",
       " 'sa2_name_2016': 0,\n",
       " 'geometry': 16018,\n",
       " 'order_month': 0,\n",
       " 'order_year': 0,\n",
       " 'sa2_code': 18225,\n",
       " 'median_aud': 18225,\n",
       " 'median_age_of_earners_years': 18225,\n",
       " 'gini_coefficient_coef': 18225,\n",
       " 'earners_persons': 18225}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_null = {col:trx_income.filter(trx_income[col].isNull()).count() \n",
    "             for col in trx_income.columns}\n",
    "dict_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f35015a8-6602-42ad-834a-08863d7c2e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38965403"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trx_income.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f35d340-5409-431a-91d3-2392e3b004f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 557:>                                                       (0 + 4) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 10:57:05 ERROR Executor: Exception in task 1.0 in stage 557.0 (TID 1817)\n",
      "java.lang.OutOfMemoryError: Cannot reserve 1073741824 bytes of direct buffer memory (allocated: 3921266348, limit: 4294967296)\n",
      "\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n",
      "\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:120)\n",
      "\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:330)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n",
      "\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n",
      "\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n",
      "\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:113)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3800)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "22/09/08 10:57:05 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 557.0 (TID 1817),5,main]\n",
      "java.lang.OutOfMemoryError: Cannot reserve 1073741824 bytes of direct buffer memory (allocated: 3921266348, limit: 4294967296)\n",
      "\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n",
      "\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:120)\n",
      "\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:330)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n",
      "\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n",
      "\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n",
      "\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:113)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3800)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "22/09/08 10:57:05 WARN TaskSetManager: Lost task 1.0 in stage 557.0 (TID 1817) (100.94.176.202 executor driver): java.lang.OutOfMemoryError: Cannot reserve 1073741824 bytes of direct buffer memory (allocated: 3921266348, limit: 4294967296)\n",
      "\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n",
      "\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:120)\n",
      "\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:330)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n",
      "\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n",
      "\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n",
      "\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:113)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3800)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "\n",
      "22/09/08 10:57:05 ERROR TaskSetManager: Task 1 in stage 557.0 failed 1 times; aborting job\n",
      "22/09/08 10:57:05 WARN TaskSetManager: Lost task 4.0 in stage 557.0 (TID 1820) (100.94.176.202 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o423.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 557.0 failed 1 times, most recent failure: Lost task 1.0 in stage 557.0 (TID 1817) (100.94.176.202 executor driver): java.lang.OutOfMemoryError: Cannot reserve 1073741824 bytes of direct buffer memory (allocated: 3921266348, limit: 4294967296)\n\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:120)\n\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:330)\n\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:113)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3800)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3798)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3802)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3779)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3779)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3778)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\nCaused by: java.lang.OutOfMemoryError: Cannot reserve 1073741824 bytes of direct buffer memory (allocated: 3921266348, limit: 4294967296)\n\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:120)\n\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:330)\n\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:113)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3800)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kn/0h56wsl91tz9kbw4pcdll7zw0000gn/T/ipykernel_1849/524774554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSAMPLE_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrx_income\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAMPLE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mtmp_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"col_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mself_destruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrowPySparkSelfDestructEnabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     batches = self.toDF(*tmp_column_names)._collect_as_arrow(\n\u001b[0m\u001b[1;32m    141\u001b[0m                         \u001b[0msplit_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_destruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36m_collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mjsocket_auth_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# Separate RecordBatches from batch order indices in results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o423.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 557.0 failed 1 times, most recent failure: Lost task 1.0 in stage 557.0 (TID 1817) (100.94.176.202 executor driver): java.lang.OutOfMemoryError: Cannot reserve 1073741824 bytes of direct buffer memory (allocated: 3921266348, limit: 4294967296)\n\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:120)\n\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:330)\n\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:113)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3800)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3798)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3802)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3779)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3779)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3778)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\nCaused by: java.lang.OutOfMemoryError: Cannot reserve 1073741824 bytes of direct buffer memory (allocated: 3921266348, limit: 4294967296)\n\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:120)\n\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:330)\n\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:113)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:121)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:97)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3800)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50634)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/patrick/opt/anaconda3/lib/python3.9/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 0.10\n",
    "df = trx_income.sample(SAMPLE_SIZE, seed=0).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7df7c3-1209-479a-ba5d-a2046edbe7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3a0de-b14a-42fe-9b28-3254f06c2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_income.groupby('merchant_abn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92762a6b-70fc-4e46-97fe-53282e5cda62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6cb09f-4bcb-458f-89de-195c6fb25557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad4d94-726b-4bf7-8815-03064f828fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "271447ed-5ec0-4690-b45a-d4c3a303bf06",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Geospatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92a902-6350-44c2-9f27-a8686205cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poa_to_sa2 = pd.read_csv(\"../data/curated/poa_w_sa2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db0769-e4a8-4bbf-931b-ed9bb175a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "poa_to_sa2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6c0a5-bad1-44bb-bda6-b91a9a843379",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_trx = (df_trx_sa2.groupby(['sa2_maincode_2016', 'order_year', 'order_month'])\n",
    "             .agg({'order_id':'count', 'dollar_value':'sum'})\n",
    "             .sort(['sa2_maincode_2016', 'order_year', 'order_month']))\n",
    "unique_cons = (df_trx_sa2.groupby(['sa2_maincode_2016', 'order_year', 'order_month'])\n",
    "               .agg(countDistinct('consumer_id'))\n",
    "               .sort(['sa2_maincode_2016', 'order_year', 'order_month']))\n",
    "unique_merc = (df_trx_sa2.groupby(['sa2_maincode_2016', 'order_year', 'order_month'])\n",
    "               .agg(countDistinct('merchant_abn'))\n",
    "               .sort(['sa2_maincode_2016', 'order_year', 'order_month']))\n",
    "\n",
    "def join_agg(sdf1, sdf2):\n",
    "    '''\n",
    "        take two dataframes and join the two dataframes\n",
    "    '''\n",
    "    sdf1 = (sdf1.alias(\"a\") \\\n",
    "               .join(sdf2, \n",
    "                     on=['sa2_maincode_2016', 'order_year', 'order_month'], \n",
    "                     how='inner')\n",
    "           )\n",
    "    return sdf1\n",
    "sa2_trx = join_agg(sa2_trx, unique_cons)\n",
    "sa2_trx = join_agg(sa2_trx, unique_merc)\n",
    "    \n",
    "# renaming a few columns\n",
    "field_name_change = {\"sum(dollar_value)\": \"total_dollar_value\", \n",
    "                     \"count(order_id)\": \"transaction_freq\",\n",
    "                     \"count(consumer_id)\": \"n_unique_consumer\",\n",
    "                     \"count(merchant_abn)\": \"n_unique_merchant\"}\n",
    "for old, new in field_name_change.items():\n",
    "    sa2_trx = sa2_trx.withColumnRenamed(old, new)\n",
    "\n",
    "cols = ['sa2_maincode_2016', 'order_year', 'order_month', 'n_unique_consumer', \n",
    "        'transaction_freq', 'total_dollar_value', 'n_unique_merchant']\n",
    "sa2_trx = sa2_trx[cols].sort(['sa2_maincode_2016', 'order_year', 'order_month'])\n",
    "\n",
    "sa2_trx = (sa2_trx.\n",
    "             withColumn('avg_sales_per_consumer', \n",
    "                        col(\"total_dollar_value\") / col(\"n_unique_consumer\")))\n",
    "\n",
    "sa2_trx = (sa2_trx.\n",
    "             withColumn('avg_sales_per_order', \n",
    "                        col(\"total_dollar_value\") / col(\"transaction_freq\")))\n",
    "\n",
    "sa2_trx = (sa2_trx.\n",
    "             withColumn('avg_sales_per_merchant', \n",
    "                        col(\"total_dollar_value\") / col(\"n_unique_merchant\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cede6-f6a2-454f-9521-310adae0185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_trx.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717f1fc-36e9-4b16-8018-5fe2490d9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poa_to_sa2 = poa_to_sa2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ba87f-3298-4ec3-adcd-cd0e68558f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "\n",
    "poa_to_sa2['geometry'] = poa_to_sa2['geometry'].astype('str').apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(poa_to_sa2, crs='epsg:4326')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9c953-6e82-4a2b-a794-3476dd88e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['geometry'] = gdf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4333daa-ccb8-43a0-9cab-c5779fcbdc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a JSON \n",
    "geoJSON = gdf[['sa2_maincode_2016', 'geometry']].drop_duplicates('sa2_maincode_2016').to_json()\n",
    "\n",
    "# print the first 300 chars of the json\n",
    "print(geoJSON[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e8a2a-c0be-4787-bfa4-3c1d728e59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_trx = sa2_trx.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ece1ce-110a-43ea-8ad4-886c58f411ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa2_trx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc21e4b-71fb-47b3-ae67-c75c256ac1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize trip_count by do_location id\n",
    "m = folium.Map(location=[-38.043995, 145.264296], tiles=\"Stamen Terrain\", zoom_start=8)\n",
    "sa2_trx_filter = sa2_trx.filter(F.col('state') == \"VIC\")\n",
    "\n",
    "custom_scale = (sa2_trx_filter['total_dollar_value'].quantile((0,0.2,0.4,0.6,0.8,1))).tolist()\n",
    "c = folium.Choropleth(\n",
    "    geo_data=geoJSON, # geoJSON \n",
    "    name='choropleth', # name of plot\n",
    "    data=sa2_trx, # data source\n",
    "    columns=['sa2_maincode_2016','total_dollar_value'], # the columns required\n",
    "    key_on='properties.sa2_maincode_2016', # this is from the geoJSON's properties\n",
    "    fill_color='YlOrRd', # color scheme\n",
    "    nan_fill_color='black',\n",
    "    legend_name='Average Sales per Transaction'\n",
    ")\n",
    "\n",
    "c.add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
